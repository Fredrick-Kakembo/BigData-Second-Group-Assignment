{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bio Big Data Analysis\n# Assignment II (Group Work)\n# Group Members\n1. Omara Isaac Emmanuel (Reg No: HD07/29395U)\n2. Kakembo Fredrick (Reg No:)\n3. Nabirye Stella Esther(Reg No:)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n1. Introduction\n2. Intuition\n3. Univariate Analysis\n4. Multivariate Analysis\n5. Data Preprocessing\n6. Modeling\n"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nHouses comprise the most common means of accumulating personal wealth for any family. However, as the measure of that value is not always clear, it becomes necessary to assess the value of the house between sales. These assessments are used not just for setting a sales price but for other purposes like second mortgages and home owners’ insurance. It is therefore necessary to make them as accurate as possible. In this work,we shall review important features which can be used to achieve accuracy in predicting housing prices. We shall employ regression models and tree based models, using various features to achieve lower Residual Sum of Squares error. While using features in a regression models, some feature engineering is required for better prediction. As these models are expected to be susceptible towards over fitting, Lasso, ridge regressions are used to reduce it.\n\nWe will start by conducting some exploratory data analysis followed by processing the data based on our findings. The goal is ultimately to accurately predict the Sales Price of houses in the test set given the data in the training set."},{"metadata":{},"cell_type":"markdown","source":"# 1). Exploratory Data Analysis\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.\n\nBefore we perform any downstream analysis, we shall explore the structure of the data and the relationships between the variables in the data set.With a detailed EDA of the ames train data set, we will first of all learn about the structure of the data and the relationships between the variables in the data set. This EDA will involve creating and reviewing many plots/graphs and considering the patterns and relationships that we shall see.After we have explored completely, we shall submit the three graphs/plots that we find most informative during the EDA process, and we will briefly explain what we have learnt from each.\n\nSo in this exploration we are going to;\n\n1. Understand the problem. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\n2. Univariable study. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n3. Multivariate study. We'll try to understand how the dependent variable and independent variables relate.\n4. Basic cleaning. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n5. Test assumptions. We'll check if our data meets the assumptions required by most multivariate techniques.\n\nReference: https://medium.com/@purnasaigudikandula/exploratory-data-analysis-beginner-univariate-bivariate-and-multivariate-habberman-dataset-2365264b751"},{"metadata":{},"cell_type":"markdown","source":"# Intuition: Getting to Know the Data\nBefore we start our more rigorous analysis, let's start by familiarizing ourselves with the data first. We don't need to look at every feature (yet), but getting a feel for some features that are immediately relevant to us may help us build some intuition and generate ideas or questions."},{"metadata":{},"cell_type":"markdown","source":"# Importing the Required Packages to be Used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting and Data manipulation\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nimport sklearn as sk\nimport seaborn as sns\nimport scipy as sy\nfrom scipy import stats\nfrom scipy.stats import norm\n\n# To read our csv files into our space\nfrom pandas import read_csv\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Our Data Sets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load the TRAIN DATA SET\ntrain = pd.read_csv(\"../input/group-assignment-ace-2020/train.csv\")\n\n# Load the TEST DATA SET\ntest = pd.read_csv(\"../input/group-assignment-ace-2020/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General Overview\nLet's begin by getting high-level look at our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How is our data shaping up?\nprint('The training dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\nprint('The test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a sneak peak at the train data\nprint('Training Data Shape: ', train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Shape of the Test Data Set # Obviously this doesnt have the Sale Price\nprint('Testing Data Shape: ', test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get information on our columns and data size\ntrain.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check missing values\ntrain.columns[train.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of 81 features, 19 features have missing values. Let's check the percentage of missing values in these columns."},{"metadata":{},"cell_type":"markdown","source":"# Checking the Percentage of these Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing Value counts in each of these columns \nmiss = train.isnull().sum()/len(train)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace=True)\nmiss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer that the variable PoolQC has 99.5% missing values followed by MiscFeature, Alley, and Fence. Let's look at a pretty picture explaining these missing values using a bar plot. "},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Missing Values Using a Bar Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values\nmissing_data = train.isnull().sum() / train.shape[0]\nmissing_data[missing_data > 0].\\\n    sort_values(ascending=True).\\\n    plot(kind='barh', figsize=(10,6))\nplt.title('Percentage of missing values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General Questions about house prices\nlet's start poking around and ask some questions based on our existing knowledge of housing in general, without yet going into the details of every feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How expensive are houses?\nprint('The cheapest house was sold for ${:,.0f} and the most expensive was sold for ${:,.0f}'.format(\n    train.SalePrice.min(), train.SalePrice.max()))\nprint('The average sales price is ${:,.0f}, while the median is ${:,.0f}'.format(\n    train.SalePrice.mean(), train.SalePrice.median()))\ntrain.SalePrice.hist(bins=75, rwidth=.8, figsize=(18,8))\nplt.title('How expensive are houses?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note that the distribution is postively skewed to the right with a great number of outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# When were the houses built?\nprint('The oldest house was built in {}. The Newest house was built in {}.'.format(\n    train.YearBuilt.min(), train.YearBuilt.max()))\ntrain.YearBuilt.hist(bins=14, rwidth=.9, figsize=(16,6))\nplt.title('When were the houses built?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nNot much action in the 80s apparently. Looks like majority of houses were built in the 50s and after, which good chunk of new houses were built after words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# When were the houses sold?\ntrain.groupby(['YrSold','MoSold']).Id.count().plot(kind='bar', figsize=(16,6))\nplt.title('When were the houses sold?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots above look quit interesting. We observe a strong seasonal pattern in the house sales, with high peaks in the month of June and July. We conclude that the dataset spans 2006 to 2010, but note that data steps mid-year in July of 2010.\n\nAt this point we are wondering if the time of year a house is sold has any effect on sales price. We'll address this question once we start our multivariate analysis later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Where are houses?\ntrain.groupby('Neighborhood').Id.count().\\\n    sort_values().\\\n    plot(kind='barh', figsize=(16,8))\nplt.title('What neighborhoods are houses in?')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like a good chunk of houses are found in North Ames, Collect Creek, and Old Town, with few houses in Bluestem, Northpark Villa and Veenker."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How big are houses\nprint('The average house has {:,.0f} sq ft of space, the median {:,.0f} sq ft'.format(\n    train.GrLivArea.mean(), train.GrLivArea.median()))\nprint('The biggest house has {:,.0f} sq ft of space, the smallest {:,.0f} sq ft'.format(\n    train.GrLivArea.max(), train.GrLivArea.min()))\ntrain.GrLivArea.hist(bins=21, rwidth=.8, figsize=(12,6))\nplt.title('How big are houses? (in sq feet)')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of the Target Variable (Sale Price)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a Histogram to study the distribution of the Sale Price which is the target variable\nsns.distplot(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the target variable SalePrice has a right-skewed distribution (Not Normally Distributed), but it is close enough for our models. We'll need to log transform this variable so that it becomes normally distributed. A normally distributed (or close to normal) target variable helps in better modeling the relationship between target and independent variables. In addition, linear algorithms assume constant variance in the error term. Alternatively, we can also confirm this skewed behavior using the skewness metric. The tail continues far to the right compared to the left side. Also, it is observed that there won’t be homes sold for less than $0). "},{"metadata":{},"cell_type":"markdown","source":"# Checking for Skewness\nSkewed data is common in data science; skew is the degree of distortion from a normal distribution. ... If the values of a certain independent variable (feature) are skewed, depending on the model, skewness may violate model assumptions (e.g. logistic regression) or may impair the interpretation of feature importance\n\nWhy do we care if the data is skewed? If the response variable is skewed like for example to the right as shown below, the model will be trained on a much larger number of priced variables, and will be less likely to successfully predict the target variable. The concept is the same as training a model on imbalanced categorical classes. If the values of a certain independent variable (feature) are skewed, depending on the model, skewness may violate model assumptions (e.g. logistic regression) or may impair the interpretation of feature importance.\n\nReference: https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for how much of the Sale Price is positively skewed to the right \nprint(\"The skewness of SalePrice is {}\".format(train['SalePrice'].skew()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming Skewed Data\nWe can address skewed variables by transforming them (i.e. applying the same function to each value). Common transformations include square root (sqrt(x)), logarithmic (log(x)), and reciprocal (1/x). We’ll apply each in Python to the right-skewed response variable Sale Price.\n\nReference: https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0"},{"metadata":{},"cell_type":"markdown","source":"# i) Square Root Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"resp = train.SalePrice\nsqrt_resp = resp**(.5)\nsns.distplot(sqrt_resp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After transforming, the data is definitely less skewed, but there is still a long right tail."},{"metadata":{},"cell_type":"markdown","source":"# ii) Reciprocal Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Reciprocal Transformation\nrecip =1/resp\nsns.distplot(resp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still not great, the above distribution is not quite symmetrical."},{"metadata":{},"cell_type":"markdown","source":"# iii) Log Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using log transformation\nlog_resp = np.log(resp)\nsns.distplot(log_resp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The log transformation seems to be the best, as the distribution of transformed sale prices is the most symmetrical."},{"metadata":{},"cell_type":"markdown","source":"# Alternatively;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can transform the target variable (Sale Price), this way Using the log transmformation\ntarget = np.log(train['SalePrice'])\nprint ('Skewness is', target.skew())\nsns.distplot(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you saw, log transformation of the target variable has helped us fixing its skewed distribution and the new distribution looks closer to the normal distribution. Since we have 80 variables, visualizing one by one wouldn't be an astute approach. Instead, we'll look at some variables based on their correlation with the target variable. However, there's a way to plot all variables at once, and we'll look at it as well. Moving forward, we'll separate numeric and categorical variables and explore this data from a different angle. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate Variables into new data frames\nnumeric_data = train.select_dtypes(include=[np.number])\ncat_data = train.select_dtypes(exclude=[np.number])\nprint (\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1],cat_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing the id variable from the numerics\nNow, we are interested to learn about the correlation behavior of numeric variables. Out of 38 variables, I presume some of them must be correlated. If found, we can later remove these correlated variables as they won't provide any useful information to the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the Id label from the column\ndel numeric_data['Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Plot showing how all Variables are related with SalePrice\ncorr = numeric_data.corr()\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we notice the last row of this map. We can see the correlation of all variables against SalePrice. As you can see, some variables seem to be strongly correlated with the target variable. Here, a numeric correlation score will help us understand the graph better. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the Correlation between Sale Price with the first 15 values\nprint (corr['SalePrice'].sort_values(ascending=False)[:15], '\\n') \n\n# For the last 5 values\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that the OverallQual feature is 79% correlated with the target variable. Overallqual feature refers to the overall material and quality of the materials of the completed house. Well, this make sense as well. People usually consider these parameters for their dream house. In addition, GrLivArea is 70% correlated with the target variable. GrLivArea refers to the living area (in sq ft.) above ground. The following variables show people also care about if the house has a garage, the area of that garage, the size of the basement area, etc.\n\nReference: http://jse.amstat.org/v16n2/datasets.pardoe.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the OverallQual Variable in detail\ntrain['OverallQual'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall quality is measured on a scale of 1 to 10. Hence, we can fairly treat it as an ordinal variable. An ordinal variable has an inherent order. For example, Rank of students in class, data collected on Likert scale, etc. Let's check the median sale price of houses with respect to OverallQual. You might be wondering, “Why median ?” We are using median because the target variable is skewed. A skewed variable has outliers and median is robust to outliers."},{"metadata":{},"cell_type":"markdown","source":"We can create such aggregated tables using pandas pivot tables quite easily. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check the mean price per quality and plot it.\npivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\npivot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot this table and understand the median behavior using a bar graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot.plot(kind='bar', color='pink')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This behavior is quite normal. As the overall quality of a house increases, its sale price also increases. Let's visualize the next correlated variable GrLivArea and understand their behavior. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# GrLivArea variable\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, here also we see a direct correlation of living area with sale price. However, we can spot an outlier value GrLivArea > 5000. I've seen outliers play a significant role in spoiling a model's performance. Hence, we'll get rid of it. If you are enjoying this activity, you can visualize other correlated variables as well. Now, we'll move forward and explore categorical features. The simplest way to understand categorical variables is using .describe() command. "},{"metadata":{},"cell_type":"markdown","source":"# Correlation in Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Having a view on the dataframe for the Categorical Variables generated\ncat_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the median sale price of a house based on its SaleCondition. SaleCondition explains the condition of sale. Not much information is given about its categories. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\nsp_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp_pivot.plot(kind='bar',color='purple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that SaleCondition Partial has the highest mean sale price. Though, due to lack of information we can't generate many insights from this data. Moving forward, like we used correlation to determine the influence of numeric features on SalePrice."},{"metadata":{},"cell_type":"markdown","source":"# Correlation between Categorical Variables and SalePrice (ANOVA test)\nSimilarly, we'll use the ANOVA test to understand the correlation between categorical variables and SalePrice. ANOVA test is a statistical technique used to determine if there exists a significant difference in the mean of groups. For example, let's say we have two variables A and B. Each of these variables has 3 levels (a1,a2,a3 and b1,b2,b3). If the mean of these levels with respect to the target variable is the same, the ANOVA test will capture this behavior and we can safely remove them.\n\nWhile using ANOVA, our hypothesis is as follows:\n\nHo - There exists no significant difference between the groups. Ha - There exists a significant difference between the groups.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, we'll define a function which calculates p values. \n# From those p values, we'll calculate a disparity score. Higher the disparity score, better the feature in predicting sale price. \n\ncat = [f for f in train.columns if train.dtypes[f] == 'object']\ndef anova(frame):\n    anv = pd.DataFrame()\n    anv['features'] = cat\n    pvals = []\n    for c in cat:\n           samples = []\n           for cls in frame[c].unique():\n                  s = frame[frame[c] == cls]['SalePrice'].values\n                  samples.append(s)\n           pval = stats.f_oneway(*samples)[1]\n           pvals.append(pval)\n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\ncat_data['SalePrice'] = train.SalePrice.values\nk = anova(cat_data) \nk['disparity'] = np.log(1./k['pval'].values) \nsns.barplot(data=k, x = 'features', y='disparity') \nplt.xticks(rotation=90) \nplt ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that among all categorical variables Neighborhood turned out to be the most important feature followed by ExterQual, KitchenQual, etc. It means that people also consider the goodness of the neighborhood, the quality of the kitchen, the quality of the material used on the exterior walls, etc. "},{"metadata":{},"cell_type":"markdown","source":"# Distribution of Numeric and Categorical Variables\nFinally, to get a quick glimpse of all variables in the data set, we shall plot histograms for all Numeric Variables to determine if all variables are skewed. For categorical variables, we'll create a boxplot and understand their nature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histograms to Visualize Numeric Varaibles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box Plots to Visualize Categoricla Variables\ndef boxplot(x,y,**kwargs):\n            sns.boxplot(x=x,y=y)\n            x = plt.xticks(rotation=90)\n\ncat = [f for f in train.columns if train.dtypes[f] == 'object']\n\np = pd.melt(train, id_vars='SalePrice', value_vars=cat)\ng = sns.FacetGrid (p, col='variable', col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, 'value','SalePrice')\ng","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above box plots, we can see that most of the variables possess outlier values. It would take us days if we start treating these outlier values one by one. Hence, for now we'll leave them as is and let our algorithm deal with them. As we know, tree-based algorithms are usually robust to outliers. "},{"metadata":{},"cell_type":"markdown","source":"\n# Additional Plot\n\nAt this point we're about ready to formally process our data and get it ready for modelling, but before we do, let's look at some cool visualization to show the relationship between two categorical and one numerical variable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A Factorplot \nfig = sns.factorplot(x='Neighborhood', y='SalePrice', hue='MSZoning', data=train, kind='swarm', size=12, aspect=1.5)\nax = fig.axes[0][0]\nax.set_yscale('log')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we're looking at sales price by neighborhood and color-coding by zoning classificaton of the sale. It looks like there is clearly some strong relationships between some neighborhoods and zoning classifications: Old Town, Brooksize, Iowa DOT and Rail Road, Meadow Village and Briardale are all predominantly in a \"Residental Medium Density\" zone, while Somerset is in a \"Floating Village Residential\" zone. We assume this might be like a gated community\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing\nIn this stage, we'll deal with outlier values, encode variables, impute missing values, and take every possible initiative which can remove inconsistencies from the data set. "},{"metadata":{},"cell_type":"markdown","source":"# i) Data Duplication\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for Duplication in our data\nprint('Train set duplicate IDs: {}'.format(train.duplicated('Id').sum()))\nprint('Test set duplicate IDs: {}'.format(test.duplicated('Id').sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ii) Removing Outliers from GrLivArea\nIf you remember, we discovered that the variable GrLivArea has outlier values. Precisely, one point crossed the 4000 mark. Let's remove that"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Outliers\ntrain.drop(train[train['GrLivArea'] > 4000].index, inplace=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initially, there were 1,460 rows and after removing outliers, we observed 4 rows were removed and now having a total of 1,456"},{"metadata":{},"cell_type":"markdown","source":"# iii) Imputation Using Mode (Handling Missing Values)\nIn statistics, imputation is the process of replacing missing data with substituted values (https://en.wikipedia.org/wiki/Imputation_(statistics)\n\nAlternatively, Missing data, or Missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data (https://en.wikipedia.org/wiki/Missing_data)\n\nIn row 666, in the test data, we found that information in variables related to 'Garage' (GarageQual, GarageCond, GarageFinish, GarageYrBlt) is missing. Let's impute them using the mode of these respective variables. \n\nReference:\n1. https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n2. https://analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing Using mode\n#stats.mode(test['GarageQual']).mode\ntest.loc[666, 'GarageQual'] = \"TA\" \n\n#stats.mode(test['GarageCond']).mode\ntest.loc[666, 'GarageCond'] = \"TA\" \n\n#stats.mode(test['GarageFinish']).mode\ntest.loc[666, 'GarageFinish'] = \"Unf\" \n\n#np.nanmedian(test['GarageYrBlt'])\ntest.loc[666, 'GarageYrBlt'] = \"1980\" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In row 1116, in test data, all garage variables are NA except GarageType. Let's mark it NA as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mark as Missing\ntest.loc[1116, 'GarageType'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# iv) Encoding Categorical Variables\nNow, we'll encode all the categorical variables. This is necessary because most ML algorithms do not accept categorical values, instead they are expected to be converted to numerical. LabelEncoder function from sklearn is used to encode variables. Let's write a function to do this;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the LabelEncoder function from sklearn\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef factorize(data, var, fill_na = None):\n      if fill_na is not None:\n            data[var].fillna(fill_na, inplace=True)\n      le.fit(data[var])\n      data[var] = le.transform(data[var])\n      return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function imputes the blank levels with mode values. The mode values are to be entered manually. Now, let's impute the missing values in LotFrontage variable using the median value of LotFrontage by Neighborhood. Such imputation strategies are built during data exploration. I suggest you spend some more time on data exploration. To do this, we should combine our train and test data so that we can modify both the data sets at once. Also, it'll save our time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the data set\nalldata = train.append(test)\nalldata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The combined data set has 2915 rows and 81 columns. Now, we'll impute the LotFrontage variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute lotfrontage by median of neighborhood\nlot_frontage_by_neighborhood = train['LotFrontage'].groupby(train['Neighborhood'])\n\nfor key, group in lot_frontage_by_neighborhood:\n                idx = (alldata['Neighborhood'] == key) & (alldata['LotFrontage'].isnull())\n                alldata.loc[idx, 'LotFrontage'] = group.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, in other numeric variables, we'll impute the missing values by zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputing Missing Values\n\nalldata[\"MasVnrArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF1\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF2\"].fillna(0, inplace=True)\nalldata[\"BsmtUnfSF\"].fillna(0, inplace=True)\nalldata[\"TotalBsmtSF\"].fillna(0, inplace=True)\nalldata[\"GarageArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFullBath\"].fillna(0, inplace=True)\nalldata[\"BsmtHalfBath\"].fillna(0, inplace=True)\nalldata[\"GarageCars\"].fillna(0, inplace=True)\nalldata[\"GarageYrBlt\"].fillna(0.0, inplace=True)\nalldata[\"PoolArea\"].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new variable (1 or 0) based on irregular count levels\n#The level with highest count is kept as 1 and rest as 0\nalldata[\"IsRegularLotShape\"] = (alldata[\"LotShape\"] == \"Reg\") * 1\nalldata[\"IsLandLevel\"] = (alldata[\"LandContour\"] == \"Lvl\") * 1\nalldata[\"IsLandSlopeGentle\"] = (alldata[\"LandSlope\"] == \"Gtl\") * 1\nalldata[\"IsElectricalSBrkr\"] = (alldata[\"Electrical\"] == \"SBrkr\") * 1\nalldata[\"IsGarageDetached\"] = (alldata[\"GarageType\"] == \"Detchd\") * 1\nalldata[\"IsPavedDrive\"] = (alldata[\"PavedDrive\"] == \"Y\") * 1\nalldata[\"HasShed\"] = (alldata[\"MiscFeature\"] == \"Shed\") * 1\nalldata[\"Remodeled\"] = (alldata[\"YearRemodAdd\"] != alldata[\"YearBuilt\"]) * 1\n\n# Did the modeling happen during the sale year?\nalldata[\"RecentRemodel\"] = (alldata[\"YearRemodAdd\"] == alldata[\"YrSold\"]) * 1\n\n# Was this house sold in the year it was built?\nalldata[\"VeryNewHouse\"] = (alldata[\"YearBuilt\"] == alldata[\"YrSold\"]) * 1\nalldata[\"Has2ndFloor\"] = (alldata[\"2ndFlrSF\"] == 0) * 1\nalldata[\"HasMasVnr\"] = (alldata[\"MasVnrArea\"] == 0) * 1\nalldata[\"HasWoodDeck\"] = (alldata[\"WoodDeckSF\"] == 0) * 1\nalldata[\"HasOpenPorch\"] = (alldata[\"OpenPorchSF\"] == 0) * 1\nalldata[\"HasEnclosedPorch\"] = (alldata[\"EnclosedPorch\"] == 0) * 1\nalldata[\"Has3SsnPorch\"] = (alldata[\"3SsnPorch\"] == 0) * 1\nalldata[\"HasScreenPorch\"] = (alldata[\"ScreenPorch\"] == 0) * 1\n\n# Setting levels with high count as 1 and the rest as 0\n# You can check for them using the value_counts function\nalldata[\"HighSeason\"] = alldata[\"MoSold\"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})\nalldata[\"NewerDwelling\"] = alldata[\"MSSubClass\"].replace({20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the Number of Resultant Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"alldata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have 100 features in the data. It means we create 19 more columns. Let's continue and create some more features. Once again, we'll combine the original train and test files to create a parallel alldata2 file. This file will have original feature values. We'll use this data as reference to create more features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create alldata2\nalldata2 = train.append(test)\n\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# house completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just like Garage, we have several columns associated with the area of the property. An interesting variable could be the sum of all areas for a particular house. In addition, we can also create new features based on the year the house built. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating total area using all area columns\narea_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n\nalldata[\"TotalArea\"] = alldata[area_cols].sum(axis=1)\nalldata[\"TotalArea1st2nd\"] = alldata[\"1stFlrSF\"] + alldata[\"2ndFlrSF\"]\nalldata[\"Age\"] = 2010 - alldata[\"YearBuilt\"]\nalldata[\"TimeSinceSold\"] = 2010 - alldata[\"YrSold\"]\nalldata[\"SeasonSold\"] = alldata[\"MoSold\"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)\nalldata[\"YearsSinceRemodel\"] = alldata[\"YrSold\"] - alldata[\"YearRemodAdd\"]\n\n# Simplifications of existing features into bad/average/good based on counts\nalldata[\"SimplOverallQual\"] = alldata.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplOverallCond\"] = alldata.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplPoolQC\"] = alldata.PoolQC.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2})\nalldata[\"SimplGarageCond\"] = alldata.GarageCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplGarageQual\"] = alldata.GarageQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFunctional\"] = alldata.Functional.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})\nalldata[\"SimplKitchenQual\"] = alldata.KitchenQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplHeatingQC\"] = alldata.HeatingQC.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtFinType1\"] = alldata.BsmtFinType1.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtFinType2\"] = alldata.BsmtFinType2.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtCond\"] = alldata.BsmtCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtQual\"] = alldata.BsmtQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterCond\"] = alldata.ExterCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterQual\"] = alldata.ExterQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n\n#grouping neighborhood variable based on this plot\ntrain['SalePrice'].groupby(train['Neighborhood']).median().sort_values().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above gives us a good hint on how to combine levels of the neighborhood variable into fewer levels. We can combine bars of somewhat equal height in one category. To do this, we'll simply create a dictionary and map it with variable values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"neighborhood_map = {\"MeadowV\" : 0, \"IDOTRR\" : 1, \"BrDale\" : 1, \"OldTown\" : 1, \"Edwards\" : 1, \"BrkSide\" : 1,\"Sawyer\" : 1, \"Blueste\" : 1, \"SWISU\" : 2, \"NAmes\" : 2, \"NPkVill\" : 2, \"Mitchel\" : 2, \"SawyerW\" : 2, \"Gilbert\" : 2, \"NWAmes\" : 2, \"Blmngtn\" : 2, \"CollgCr\" : 2, \"ClearCr\" : 3, \"Crawfor\" : 3, \"Veenker\" : 3, \"Somerst\" : 3, \"Timber\" : 3, \"StoneBr\" : 4, \"NoRidge\" : 4, \"NridgHt\" : 4}\n\nalldata['NeighborhoodBin'] = alldata2['Neighborhood'].map(neighborhood_map)\nalldata.loc[alldata2.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\nalldata[\"Neighborhood_Good\"].fillna(0, inplace=True)\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# House completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})\nalldata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Until this point, we've added 43 new features in the data set. Now, let's split the data into test and train and create some more features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new data\ntrain_new = alldata[alldata['SalePrice'].notnull()]\nprint (train, train_new.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating New Data\ntest_new = alldata[alldata['SalePrice'].isnull()]\nprint (test, test_new.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform Numeric features and remove their skewness. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get Numeric Features\nnumeric_features = [f for f in train_new.columns if train_new[f].dtype != object]\n\n#transform the numeric features using log(x + 1)\nfrom scipy.stats import skew\nskewed = train_new[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\ntrain_new[skewed] = np.log1p(train_new[skewed])\ntest_new[skewed] = np.log1p(test_new[skewed])\ndel test_new['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardize Numeric Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_new[numeric_features])\nscaled = scaler.transform(train_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n       train_new[col] = scaled[:,i]\n\nnumeric_features.remove('SalePrice')\nscaled = scaler.fit_transform(test_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n      test_new[col] = scaled[:,i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll one-hot encode the categorical variable. In one-hot encoding, every level of a categorical variable results in a new variable with binary values (0 or 1). We'll write a function to encode categorical variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot(onehot_df, df, column_name, fill_na):\n       onehot_df[column_name] = df[column_name]\n       if fill_na is not None:\n            onehot_df[column_name].fillna(fill_na, inplace=True)\n\n       dummies = pd.get_dummies(onehot_df[column_name], prefix=\"_\"+column_name)\n       onehot_df = onehot_df.join(dummies)\n       onehot_df = onehot_df.drop([column_name], axis=1)\n       return onehot_df\n\ndef munge_onehot(df):\n       onehot_df = pd.DataFrame(index = df.index)\n\n       onehot_df = onehot(onehot_df, df, \"MSSubClass\", None)\n       onehot_df = onehot(onehot_df, df, \"MSZoning\", \"RL\")\n       onehot_df = onehot(onehot_df, df, \"LotConfig\", None)\n       onehot_df = onehot(onehot_df, df, \"Neighborhood\", None)\n       onehot_df = onehot(onehot_df, df, \"Condition1\", None)\n       onehot_df = onehot(onehot_df, df, \"BldgType\", None)\n       onehot_df = onehot(onehot_df, df, \"HouseStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"Exterior1st\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Exterior2nd\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Foundation\", None)\n       onehot_df = onehot(onehot_df, df, \"SaleType\", \"WD\")\n       onehot_df = onehot(onehot_df, df, \"SaleCondition\", \"Normal\")\n\n       #Fill in missing MasVnrType for rows that do have a MasVnrArea.\n       temp_df = df[[\"MasVnrType\", \"MasVnrArea\"]].copy()\n       idx = (df[\"MasVnrArea\"] != 0) & ((df[\"MasVnrType\"] == \"None\") | (df[\"MasVnrType\"].isnull()))\n       temp_df.loc[idx, \"MasVnrType\"] = \"BrkFace\"\n       onehot_df = onehot(onehot_df, temp_df, \"MasVnrType\", \"None\")\n\n       onehot_df = onehot(onehot_df, df, \"LotShape\", None)\n       onehot_df = onehot(onehot_df, df, \"LandContour\", None)\n       onehot_df = onehot(onehot_df, df, \"LandSlope\", None)\n       onehot_df = onehot(onehot_df, df, \"Electrical\", \"SBrkr\")\n       onehot_df = onehot(onehot_df, df, \"GarageType\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PavedDrive\", None)\n       onehot_df = onehot(onehot_df, df, \"MiscFeature\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Street\", None)\n       onehot_df = onehot(onehot_df, df, \"Alley\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Condition2\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofMatl\", None)\n       onehot_df = onehot(onehot_df, df, \"Heating\", None)\n\n       # we'll have these as numerical variables too\n       onehot_df = onehot(onehot_df, df, \"ExterQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"ExterCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"HeatingQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"KitchenQual\", \"TA\")\n       onehot_df = onehot(onehot_df, df, \"FireplaceQu\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PoolQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtExposure\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType1\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType2\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Functional\", \"Typ\")\n       onehot_df = onehot(onehot_df, df, \"GarageFinish\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Fence\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"MoSold\", None)\n\n       # Divide  the years between 1871 and 2010 into slices of 20 years\n       year_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20))  for i in range(0, 7))\n       yearbin_df = pd.DataFrame(index = df.index)\n       yearbin_df[\"GarageYrBltBin\"] = df.GarageYrBlt.map(year_map)\n       yearbin_df[\"GarageYrBltBin\"].fillna(\"NoGarage\", inplace=True)\n       yearbin_df[\"YearBuiltBin\"] = df.YearBuilt.map(year_map)\n       yearbin_df[\"YearRemodAddBin\"] = df.YearRemodAdd.map(year_map)\n\n       onehot_df = onehot(onehot_df, yearbin_df, \"GarageYrBltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearBuiltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearRemodAddBin\", None)\n       return onehot_df\n\n#create one-hot features\nonehot_df = munge_onehot(train)\n\nneighborhood_train = pd.DataFrame(index=train_new.shape)\nneighborhood_train['NeighborhoodBin'] = train_new['NeighborhoodBin']\nneighborhood_test = pd.DataFrame(index=test_new.shape)\nneighborhood_test['NeighborhoodBin'] = test_new['NeighborhoodBin']\n\nonehot_df = onehot(onehot_df, neighborhood_train, 'NeighborhoodBin', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add the one-hot variables in our train data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new = train_new.join(onehot_df) \ntrain_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, this resulted into 433 more columns. Similarly, we will add one-hot variables in test data as well. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding one hot features to test\nonehot_df_te = munge_onehot(test)\nonehot_df_te = onehot(onehot_df_te, neighborhood_test, \"NeighborhoodBin\", None)\ntest_new = test_new.join(onehot_df_te)\ntest_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference in number of train and test columns suggests that some new features in the train data aren't available in the test data. Let's remove those variables and keep an equal number of columns in the train and test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping some columns from the train data as they are not found in test\ndrop_cols = [\"_Exterior1st_ImStucc\", \"_Exterior1st_Stone\",\"_Exterior2nd_Other\",\"_HouseStyle_2.5Fin\",\"_RoofMatl_Membran\", \"_RoofMatl_Metal\", \"_RoofMatl_Roll\", \"_Condition2_RRAe\", \"_Condition2_RRAn\", \"_Condition2_RRNn\", \"_Heating_Floor\", \"_Heating_OthW\", \"_Electrical_Mix\", \"_MiscFeature_TenC\", \"_GarageQual_Ex\",  \"_PoolQC_Fa\"]\ntrain_new.drop(drop_cols, axis=1, inplace=True)\ntrain_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have an equal number of columns in the train and test data. Here, we'll remove a few more columns which either have lots of zeroes (hence doesn't provide any real information) or aren't available in either of the data sets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop these columns\ndrop_cols = [\"_Condition2_PosN\", # only two are not zero\n         \"_MSZoning_C (all)\",\n         \"_MSSubClass_160\"]\n\ntrain_new.drop(drop_cols, axis=1, inplace=True)\ntest_new.drop(drop_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References for the Assignment\n1. https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0\n2. http://jse.amstat.org/v16n2/datasets.pardoe.html"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}